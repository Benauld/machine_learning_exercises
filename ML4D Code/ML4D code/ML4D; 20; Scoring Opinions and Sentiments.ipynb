{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scoring Opinions and Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding How Machines Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_1 = 'The quick brown fox jumps over the lazy dog.'\n",
    "text_2 = 'My dog is quick and can jump over fences.'\n",
    "text_3 = 'Your dog is so lazy that it sleeps all the day.'\n",
    "corpus = [text_1, text_2, text_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0]\n",
      " [1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "vectorizer = text.CountVectorizer(binary=True).fit(corpus)\n",
    "vectorized_text = vectorizer.transform(corpus)\n",
    "print(vectorized_text.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'day': 4, 'jumps': 11, 'that': 18, 'the': 19, 'is': 8, 'fences': 6, 'lazy': 12, 'and': 1, 'quick': 15, 'my': 13, 'can': 3, 'it': 9, 'so': 17, 'all': 0, 'brown': 2, 'dog': 5, 'jump': 10, 'over': 14, 'sleeps': 16, 'your': 20, 'fox': 7}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Enhancing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 1 0 0 2 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "text_4 = 'A black dog just passed by but my dog is brown.'\n",
    "corpus.append(text_4)\n",
    "vectorizer = text.CountVectorizer().fit(corpus)\n",
    "vectorized_text = vectorizer.transform(corpus)\n",
    "print(vectorized_text.todense()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        is: 0.077\n",
      "        by: 0.121\n",
      "     brown: 0.095\n",
      "       dog: 0.126\n",
      "      just: 0.121\n",
      "        my: 0.095\n",
      "     black: 0.121\n",
      "    passed: 0.121\n",
      "       but: 0.121\n",
      "\n",
      "Summed values of a phrase: 1.0\n"
     ]
    }
   ],
   "source": [
    "TfidF = text.TfidfTransformer(norm='l1')\n",
    "tfidf = TfidF.fit_transform(vectorized_text)\n",
    "\n",
    "phrase = 3 # choose a number from 0 to 3\n",
    "total = 0\n",
    "for word in vectorizer.vocabulary_:\n",
    "    pos = vectorizer.vocabulary_[word]\n",
    "    value = list(tfidf.toarray()[phrase])[pos]\n",
    "    if value !=0:\n",
    "        print (\"%10s: %0.3f\" % (word, value))\n",
    "        total += value\n",
    "print ('\\nSummed values of a phrase: %0.1f' % total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'can jump': 6, 'by but': 5, 'over the': 21, 'it sleeps': 13, 'your dog': 31, 'the quick': 30, 'and can': 1, 'so lazy': 26, 'is so': 12, 'dog is': 7, 'quick brown': 24, 'lazy dog': 17, 'fox jumps': 9, 'is brown': 10, 'my dog': 19, 'passed by': 22, 'lazy that': 18, 'black dog': 2, 'brown fox': 3, 'that it': 27, 'quick and': 23, 'the day': 28, 'just passed': 16, 'dog just': 8, 'jump over': 14, 'sleeps all': 25, 'over fences': 20, 'jumps over': 15, 'the lazy': 29, 'but my': 4, 'all the': 0, 'is quick': 11}\n"
     ]
    }
   ],
   "source": [
    "bigrams = text.CountVectorizer(ngram_range=(2,2))\n",
    "print (bigrams.fit(corpus).vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Luca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "['love', 'sam', 'swim', 'time']\n",
      "[[1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vocab = ['Sam loves swimming so he swims all the time']\n",
    "vect = text.CountVectorizer(tokenizer=tokenize, \n",
    "                           stop_words='english')\n",
    "vec = vect.fit(vocab)\n",
    "\n",
    "sentence1 = vec.transform(['George loves swimming too!'])\n",
    "\n",
    "print (vec.get_feature_names())\n",
    "print (sentence1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Textual Datasets from the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "try:\n",
    "    import urllib2 # Python 2.7.x\n",
    "except:\n",
    "    import urllib.request as urllib2 # Python 3.x\n",
    "\n",
    "wiki = \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\"\n",
    "header = {'User-Agent': 'Mozilla/5.0'} \n",
    "query = urllib2.Request(wiki, headers=header)\n",
    "page = urllib2.urlopen(query)\n",
    "soup = BeautifulSoup(page, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table = soup.find(\"table\", { \"class\" : \"wikitable sortable\" })\n",
    "final_table = list()\n",
    "for row in table.findAll('tr'):\n",
    "    cells = row.findAll(\"td\")\n",
    "    if len(cells) >=6:\n",
    "        v1 = cells[1].find(text=True)\n",
    "        v2 = cells[2].find(text=True)\n",
    "        v3 = cells[3].find(text=True)\n",
    "        v4 = cells[4].find(text=True)\n",
    "        v5 = cells[6].findAll(text=True)\n",
    "        v5 = v5[2].split()[0]\n",
    "        final_table.append([v1, v2, v3, v4, v5])\n",
    "cols = ['City','State','Population_2014','Census_2010'\n",
    "        ,'Land_Area_km2']\n",
    "df = pd.DataFrame(final_table, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Population_2014</th>\n",
       "      <th>Census_2010</th>\n",
       "      <th>Land_Area_km2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>8,491,079</td>\n",
       "      <td>8,175,133</td>\n",
       "      <td>783.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>3,928,864</td>\n",
       "      <td>3,792,621</td>\n",
       "      <td>1,213.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chicago</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>2,722,389</td>\n",
       "      <td>2,695,598</td>\n",
       "      <td>589.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Houston</td>\n",
       "      <td>Texas</td>\n",
       "      <td>2,239,558</td>\n",
       "      <td>2,100,263</td>\n",
       "      <td>1,552.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1,560,297</td>\n",
       "      <td>1,526,006</td>\n",
       "      <td>347.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>1,537,058</td>\n",
       "      <td>1,445,632</td>\n",
       "      <td>1,338.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1,436,697</td>\n",
       "      <td>1,327,407</td>\n",
       "      <td>1,193.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>San Diego</td>\n",
       "      <td>California</td>\n",
       "      <td>1,381,069</td>\n",
       "      <td>1,307,402</td>\n",
       "      <td>842.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dallas</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1,281,047</td>\n",
       "      <td>1,197,816</td>\n",
       "      <td>881.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>San Jose</td>\n",
       "      <td>California</td>\n",
       "      <td>1,015,785</td>\n",
       "      <td>945,942</td>\n",
       "      <td>457.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Austin</td>\n",
       "      <td>Texas</td>\n",
       "      <td>912,791</td>\n",
       "      <td>790,390</td>\n",
       "      <td>835.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Jacksonville</td>\n",
       "      <td>Florida</td>\n",
       "      <td>853,382</td>\n",
       "      <td>821,784</td>\n",
       "      <td>1,934.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>852,469</td>\n",
       "      <td>805,235</td>\n",
       "      <td>121.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>848,788</td>\n",
       "      <td>820,445</td>\n",
       "      <td>936.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Columbus</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>835,957</td>\n",
       "      <td>787,033</td>\n",
       "      <td>562.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>Texas</td>\n",
       "      <td>812,238</td>\n",
       "      <td>741,206</td>\n",
       "      <td>880.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>809,958</td>\n",
       "      <td>731,424</td>\n",
       "      <td>771.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Detroit</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>680,250</td>\n",
       "      <td>713,777</td>\n",
       "      <td>359.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>El Paso</td>\n",
       "      <td>Texas</td>\n",
       "      <td>679,036</td>\n",
       "      <td>649,121</td>\n",
       "      <td>661.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>Washington</td>\n",
       "      <td>668,342</td>\n",
       "      <td>608,660</td>\n",
       "      <td>217.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Denver</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>663,862</td>\n",
       "      <td>600,158</td>\n",
       "      <td>396.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Washington</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>658,893</td>\n",
       "      <td>601,723</td>\n",
       "      <td>158.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Memphis</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>656,861</td>\n",
       "      <td>646,889</td>\n",
       "      <td>816.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Boston</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>655,884</td>\n",
       "      <td>617,594</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Nashville</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>644,014</td>\n",
       "      <td>601,222</td>\n",
       "      <td>1,230.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Baltimore</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>622,793</td>\n",
       "      <td>620,961</td>\n",
       "      <td>209.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Oklahoma City</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>620,602</td>\n",
       "      <td>579,999</td>\n",
       "      <td>1,570.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Portland</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>619,360</td>\n",
       "      <td>583,776</td>\n",
       "      <td>345.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>613,599</td>\n",
       "      <td>583,756</td>\n",
       "      <td>351.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Louisville</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>612,780</td>\n",
       "      <td>597,337</td>\n",
       "      <td>842.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Murrieta</td>\n",
       "      <td>California</td>\n",
       "      <td>108,368</td>\n",
       "      <td>103,466</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>Centennial</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>107,201</td>\n",
       "      <td>100,377</td>\n",
       "      <td>74.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Norwalk</td>\n",
       "      <td>California</td>\n",
       "      <td>107,096</td>\n",
       "      <td>105,549</td>\n",
       "      <td>25.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>North Charleston</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>106,749</td>\n",
       "      <td>97,471</td>\n",
       "      <td>119.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>Everett</td>\n",
       "      <td>Washington</td>\n",
       "      <td>106,736</td>\n",
       "      <td>103,019</td>\n",
       "      <td>86.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>Pompano Beach</td>\n",
       "      <td>Florida</td>\n",
       "      <td>106,105</td>\n",
       "      <td>99,845</td>\n",
       "      <td>62.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>Daly City</td>\n",
       "      <td>California</td>\n",
       "      <td>106,094</td>\n",
       "      <td>101,123</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>Palm Bay</td>\n",
       "      <td>Florida</td>\n",
       "      <td>105,838</td>\n",
       "      <td>103,190</td>\n",
       "      <td>170.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Burbank</td>\n",
       "      <td>California</td>\n",
       "      <td>105,368</td>\n",
       "      <td>103,340</td>\n",
       "      <td>44.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>Wichita Falls</td>\n",
       "      <td>Texas</td>\n",
       "      <td>105,114</td>\n",
       "      <td>104,553</td>\n",
       "      <td>186.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Boulder</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>105,112</td>\n",
       "      <td>97,385</td>\n",
       "      <td>66.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>Green Bay</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>104,891</td>\n",
       "      <td>104,057</td>\n",
       "      <td>117.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>Broken Arrow</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>104,726</td>\n",
       "      <td>98,850</td>\n",
       "      <td>159.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>West Palm Beach</td>\n",
       "      <td>Florida</td>\n",
       "      <td>104,031</td>\n",
       "      <td>99,919</td>\n",
       "      <td>143.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>College Station</td>\n",
       "      <td>Texas</td>\n",
       "      <td>103,483</td>\n",
       "      <td>93,857</td>\n",
       "      <td>128.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>Pearland</td>\n",
       "      <td>Texas</td>\n",
       "      <td>103,441</td>\n",
       "      <td>91,252</td>\n",
       "      <td>122.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>Santa Maria</td>\n",
       "      <td>California</td>\n",
       "      <td>103,410</td>\n",
       "      <td>99,553</td>\n",
       "      <td>58.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>El Cajon</td>\n",
       "      <td>California</td>\n",
       "      <td>103,091</td>\n",
       "      <td>99,478</td>\n",
       "      <td>37.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>San Mateo</td>\n",
       "      <td>California</td>\n",
       "      <td>102,893</td>\n",
       "      <td>97,207</td>\n",
       "      <td>41.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>Lewisville</td>\n",
       "      <td>Texas</td>\n",
       "      <td>102,889</td>\n",
       "      <td>95,290</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>Rialto</td>\n",
       "      <td>California</td>\n",
       "      <td>102,741</td>\n",
       "      <td>99,171</td>\n",
       "      <td>57.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>Davenport</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>102,448</td>\n",
       "      <td>99,685</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>Lakeland</td>\n",
       "      <td>Florida</td>\n",
       "      <td>102,346</td>\n",
       "      <td>97,422</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>Clovis</td>\n",
       "      <td>California</td>\n",
       "      <td>102,189</td>\n",
       "      <td>95,631</td>\n",
       "      <td>60.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Edison</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>101,970</td>\n",
       "      <td>99,967</td>\n",
       "      <td>79.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Sandy Springs</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>101,908</td>\n",
       "      <td>93,853</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>Tyler</td>\n",
       "      <td>Texas</td>\n",
       "      <td>101,421</td>\n",
       "      <td>96,900</td>\n",
       "      <td>140.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>Las Cruces</td>\n",
       "      <td>New Mexico</td>\n",
       "      <td>101,408</td>\n",
       "      <td>97,618</td>\n",
       "      <td>197.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>South Bend</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>101,190</td>\n",
       "      <td>101,168</td>\n",
       "      <td>107.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Woodbridge</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>100,824</td>\n",
       "      <td>99,585</td>\n",
       "      <td>63.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 City                 State Population_2014 Census_2010  \\\n",
       "0            New York              New York       8,491,079   8,175,133   \n",
       "1         Los Angeles            California       3,928,864   3,792,621   \n",
       "2             Chicago              Illinois       2,722,389   2,695,598   \n",
       "3             Houston                 Texas       2,239,558   2,100,263   \n",
       "4        Philadelphia          Pennsylvania       1,560,297   1,526,006   \n",
       "5             Phoenix               Arizona       1,537,058   1,445,632   \n",
       "6         San Antonio                 Texas       1,436,697   1,327,407   \n",
       "7           San Diego            California       1,381,069   1,307,402   \n",
       "8              Dallas                 Texas       1,281,047   1,197,816   \n",
       "9            San Jose            California       1,015,785     945,942   \n",
       "10             Austin                 Texas         912,791     790,390   \n",
       "11       Jacksonville               Florida         853,382     821,784   \n",
       "12      San Francisco            California         852,469     805,235   \n",
       "13       Indianapolis               Indiana         848,788     820,445   \n",
       "14           Columbus                  Ohio         835,957     787,033   \n",
       "15         Fort Worth                 Texas         812,238     741,206   \n",
       "16          Charlotte        North Carolina         809,958     731,424   \n",
       "17            Detroit              Michigan         680,250     713,777   \n",
       "18            El Paso                 Texas         679,036     649,121   \n",
       "19            Seattle            Washington         668,342     608,660   \n",
       "20             Denver              Colorado         663,862     600,158   \n",
       "21         Washington  District of Columbia         658,893     601,723   \n",
       "22            Memphis             Tennessee         656,861     646,889   \n",
       "23             Boston         Massachusetts         655,884     617,594   \n",
       "24          Nashville             Tennessee         644,014     601,222   \n",
       "25          Baltimore              Maryland         622,793     620,961   \n",
       "26      Oklahoma City              Oklahoma         620,602     579,999   \n",
       "27           Portland                Oregon         619,360     583,776   \n",
       "28          Las Vegas                Nevada         613,599     583,756   \n",
       "29         Louisville              Kentucky         612,780     597,337   \n",
       "..                ...                   ...             ...         ...   \n",
       "267          Murrieta            California         108,368     103,466   \n",
       "268        Centennial              Colorado         107,201     100,377   \n",
       "269           Norwalk            California         107,096     105,549   \n",
       "270  North Charleston        South Carolina         106,749      97,471   \n",
       "271           Everett            Washington         106,736     103,019   \n",
       "272     Pompano Beach               Florida         106,105      99,845   \n",
       "273         Daly City            California         106,094     101,123   \n",
       "274          Palm Bay               Florida         105,838     103,190   \n",
       "275           Burbank            California         105,368     103,340   \n",
       "276     Wichita Falls                 Texas         105,114     104,553   \n",
       "277           Boulder              Colorado         105,112      97,385   \n",
       "278         Green Bay             Wisconsin         104,891     104,057   \n",
       "279      Broken Arrow              Oklahoma         104,726      98,850   \n",
       "280   West Palm Beach               Florida         104,031      99,919   \n",
       "281   College Station                 Texas         103,483      93,857   \n",
       "282          Pearland                 Texas         103,441      91,252   \n",
       "283       Santa Maria            California         103,410      99,553   \n",
       "284          El Cajon            California         103,091      99,478   \n",
       "285         San Mateo            California         102,893      97,207   \n",
       "286        Lewisville                 Texas         102,889      95,290   \n",
       "287            Rialto            California         102,741      99,171   \n",
       "288         Davenport                  Iowa         102,448      99,685   \n",
       "289          Lakeland               Florida         102,346      97,422   \n",
       "290            Clovis            California         102,189      95,631   \n",
       "291            Edison            New Jersey         101,970      99,967   \n",
       "292     Sandy Springs               Georgia         101,908      93,853   \n",
       "293             Tyler                 Texas         101,421      96,900   \n",
       "294        Las Cruces            New Mexico         101,408      97,618   \n",
       "295        South Bend               Indiana         101,190     101,168   \n",
       "296        Woodbridge            New Jersey         100,824      99,585   \n",
       "\n",
       "    Land_Area_km2  \n",
       "0           783.8  \n",
       "1         1,213.9  \n",
       "2           589.6  \n",
       "3         1,552.9  \n",
       "4           347.3  \n",
       "5         1,338.3  \n",
       "6         1,193.8  \n",
       "7           842.2  \n",
       "8           881.9  \n",
       "9           457.3  \n",
       "10          835.2  \n",
       "11        1,934.7  \n",
       "12          121.4  \n",
       "13          936.1  \n",
       "14          562.5  \n",
       "15          880.1  \n",
       "16          771.0  \n",
       "17          359.4  \n",
       "18          661.1  \n",
       "19          217.4  \n",
       "20          396.3  \n",
       "21          158.1  \n",
       "22          816.0  \n",
       "23          125.0  \n",
       "24        1,230.8  \n",
       "25          209.6  \n",
       "26        1,570.6  \n",
       "27          345.6  \n",
       "28          351.8  \n",
       "29          842.4  \n",
       "..            ...  \n",
       "267          87.0  \n",
       "268          74.4  \n",
       "269          25.1  \n",
       "270         119.5  \n",
       "271          86.6  \n",
       "272          62.2  \n",
       "273          19.8  \n",
       "274         170.2  \n",
       "275          44.9  \n",
       "276         186.8  \n",
       "277          66.5  \n",
       "278         117.8  \n",
       "279         159.5  \n",
       "280         143.2  \n",
       "281         128.5  \n",
       "282         122.9  \n",
       "283          58.9  \n",
       "284          37.4  \n",
       "285          41.1  \n",
       "286         110.0  \n",
       "287          57.9  \n",
       "288         163.0  \n",
       "289         193.0  \n",
       "290          60.3  \n",
       "291          79.4  \n",
       "292         101.0  \n",
       "293         140.8  \n",
       "294         197.6  \n",
       "295         107.4  \n",
       "296          63.5  \n",
       "\n",
       "[297 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Using Scoring and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sklearn.datasets.twenty_newsgroups:Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts: 585\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, \n",
    "    categories = ['misc.forsale'],\n",
    "     remove=('headers', 'footers', 'quotes'), random_state=101)\n",
    "print ('Posts: %i' % len(dataset.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, \n",
    "            min_df=2, stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(dataset.data)\n",
    "from sklearn.decomposition import NMF\n",
    "n_topics = 5\n",
    "nmf = NMF(n_components=n_topics, random_state=101).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "drive hard card floppy monitor meg ram disk motherboard vga scsi brand color internal modem\n",
      "Topic #2:\n",
      "00 50 dos 20 10 15 cover 1st new 25 price man 40 shipping comics\n",
      "Topic #3:\n",
      "condition excellent offer asking best car old sale good new miles 10 000 tape cd\n",
      "Topic #4:\n",
      "email looking games game mail interested send like thanks price package list sale want know\n",
      "Topic #5:\n",
      "shipping vcr stereo works obo included amp plus great volume vhs unc mathes gibbs radley\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "n_top_words = 15\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "   print (\"Topic #%d:\" % (topic_idx+1),)\n",
    "   print (\" \".join([feature_names[i] for i in \n",
    "                    topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1337 1749  889 1572 2342 2263 2803 1290 2353 3615 3017  806 1022 1938 2334]\n"
     ]
    }
   ],
   "source": [
    "print (nmf.components_[0,:].argsort()[:-n_top_words-1:-1]) \n",
    "# Gets top words for topic 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.get_feature_names()[1337]) \n",
    "# Transforms index 1337 back to text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analyzing reviews from e-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting in C:\\Users\\Luca\\SciPkg\\WinPython-64bit-3.4.4.1\\notebooks\\ML4D code\n",
      "\tunzipping C:\\Users\\Luca\\SciPkg\\WinPython-64bit-3.4.4.1\\notebooks\\ML4D code\\amazon_cells_labelled.txt\n",
      "\tunzipping C:\\Users\\Luca\\SciPkg\\WinPython-64bit-3.4.4.1\\notebooks\\ML4D code\\imdb_labelled.txt\n",
      "\tunzipping C:\\Users\\Luca\\SciPkg\\WinPython-64bit-3.4.4.1\\notebooks\\ML4D code\\readme.txt\n",
      "\tunzipping C:\\Users\\Luca\\SciPkg\\WinPython-64bit-3.4.4.1\\notebooks\\ML4D code\\yelp_labelled.txt\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import urllib2 # Python 2.7.x\n",
    "except:\n",
    "    import urllib.request as urllib2 # Python 3.x\n",
    "import requests, io, os, zipfile\n",
    "\n",
    "UCI_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip'\n",
    "response = requests.get(UCI_url)\n",
    "compressed_file = io.BytesIO(response.content)\n",
    "z = zipfile.ZipFile(compressed_file)\n",
    "print ('Extracting in %s' %  os.getcwd())\n",
    "for name in z.namelist():\n",
    "    filename = name.split('/')[-1]\n",
    "    nameOK = ('MACOSX' not in name and '.DS' not in name)\n",
    "    if filename and nameOK:\n",
    "            newfile = os.path.join(os.getcwd(), \n",
    "                                   os.path.basename(filename))\n",
    "            with open(newfile, 'wb') as f:\n",
    "                f.write(z.read(name))\n",
    "            print ('\\tunzipping %s' % newfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "dataset = 'imdb_labelled.txt'\n",
    "data = pd.read_csv(dataset, header=None, sep=r\"\\t\", engine='python')\n",
    "data.columns = ['review','sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  A very, very, very slow-moving, aimless movie ...          0\n",
       "1  Not sure who was more lost - the flat characte...          0\n",
       "2  Attempting artiness with black & white and cle...          0\n",
       "3       Very little music or anything to speak of.            0\n",
       "4  The best scene in the movie was when Gerardo i...          1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "corpus, test_corpus, y, yt = train_test_split(data.ix[:,0], data.ix[:,1], test_size=0.25, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "vectorizer = text.CountVectorizer(ngram_range=(1,2), \n",
    "                    stop_words='english').fit(corpus)\n",
    "TfidF = text.TfidfTransformer()\n",
    "X = TfidF.fit_transform(vectorizer.transform(corpus))\n",
    "Xt = TfidF.transform(vectorizer.transform(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = {'C': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "clf = GridSearchCV(LinearSVC(loss='hinge', \n",
    "                    random_state=101), param_grid)\n",
    "clf = clf.fit(X, y)\n",
    "print (\"Best parameters: %s\" % clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achieved accuracy: 0.816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "solution = clf.predict(Xt)\n",
    "print(\"Achieved accuracy: %0.3f\" % \n",
    "      accuracy_score(yt, solution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601    There is simply no excuse for something this p...\n",
      "32     This is the kind of money that is wasted prope...\n",
      "887    At any rate this film stinks, its not funny, a...\n",
      "668    Speaking of the music, it is unbearably predic...\n",
      "408         It really created a unique feeling though.  \n",
      "413         The camera really likes her in this movie.  \n",
      "138    I saw \"Mirrormask\" last night and it was an un...\n",
      "132    This was a poor remake of \"My Best Friends Wed...\n",
      "291                               Rating: 1 out of 10.  \n",
      "904    I'm so sorry but I really can't recommend it t...\n",
      "410    A world better than 95% of the garbage in the ...\n",
      "55     But I recommend waiting for their future effor...\n",
      "826    The film deserves strong kudos for taking this...\n",
      "100            I don't think you will be disappointed.  \n",
      "352                                    It is shameful.  \n",
      "171    This movie now joins Revenge of the Boogeyman ...\n",
      "814    You share General Loewenhielm's exquisite joy ...\n",
      "218    It's this pandering to the audience that sabot...\n",
      "168    Still, I do like this movie for it's empowerme...\n",
      "479                     Of course, the acting is blah.  \n",
      "31                      Waste your money on this game.  \n",
      "805    The only place good for this film is in the ga...\n",
      "127    My only problem is I thought the actor playing...\n",
      "613                                       Go watch it!  \n",
      "764                      This movie is also revealing.  \n",
      "107    I love Lane, but I've never seen her in a movi...\n",
      "674    Tom Wilkinson broke my heart at the end... and...\n",
      "30     There are massive levels, massive unlockable c...\n",
      "667                                    It is not good.  \n",
      "823    I struggle to find anything bad to say about i...\n",
      "739         What on earth is Irons doing in this film?  \n",
      "185                              Highly unrecommended.  \n",
      "621    A mature, subtle script that suggests and occa...\n",
      "462    Considering the relations off screen between T...\n",
      "595    Easily, none other cartoon made me laugh in a ...\n",
      "8                                   A bit predictable.  \n",
      "446    I like Armand Assante & my cable company's sum...\n",
      "449    I won't say any more - I don't like spoilers, ...\n",
      "715    Im big fan of RPG games too, but this movie, i...\n",
      "241    This would not even be good as a made for TV f...\n",
      "471    At no point in the proceedings does it look re...\n",
      "481    And, FINALLY, after all that, we get to an end...\n",
      "104                           Too politically correct.  \n",
      "522    Rating: 0/10 (Grade: Z) Note: The Show Is So B...\n",
      "174               This film has no redeeming features.  \n",
      "491    This movie creates its own universe, and is fa...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[yt!=solution])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
